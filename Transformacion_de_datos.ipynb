{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Transformacion de datos.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lesterrsantos/WebExample/blob/main/Transformacion_de_datos.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9lHkXcpaeuoW"
      },
      "source": [
        "pip install pyspark==3.2.0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-V6lt0q4fT5J"
      },
      "source": [
        "import pyspark\n",
        "from pyspark.sql import SparkSession, SQLContext\n",
        "sc = SparkSession.builder.appName(\"OperacionesDatos\").getOrCreate()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XCegV1eUeiR2",
        "outputId": "c63244d9-e1b4-44ad-f094-b00ff056899b"
      },
      "source": [
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.feature import CountVectorizer\n",
        "from pyspark.ml.feature import HashingTF, IDF, Tokenizer\n",
        "import numpy as np\n",
        "\n",
        "sentenceData = sc.createDataFrame([\n",
        "    (0.0, \"Hi I heard about Spark\"),\n",
        "    (0.0, \"I wish Spark could use case classes\"),\n",
        "    (1.0, \"Logistic regression models are neat\"),\n",
        "    (1.0, \"I love Apache Spark and ML models\")\n",
        "], [\"label\", \"sentence\"])\n",
        "sentenceData.show(truncate=False)\n",
        "\n",
        "tokenizer = Tokenizer(inputCol=\"sentence\", outputCol=\"words\")\n",
        "vectorizer  = HashingTF(inputCol=\"words\", outputCol=\"rawFeatures\", numFeatures=5)\n",
        "\n",
        "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\n",
        "\n",
        "pipeline = Pipeline(stages=[tokenizer, vectorizer, idf])\n",
        "\n",
        "\n",
        "model = pipeline.fit(sentenceData)\n",
        "model.transform(sentenceData).show(truncate=False)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+-----------------------------------+\n",
            "|label|sentence                           |\n",
            "+-----+-----------------------------------+\n",
            "|0.0  |Hi I heard about Spark             |\n",
            "|0.0  |I wish Spark could use case classes|\n",
            "|1.0  |Logistic regression models are neat|\n",
            "|1.0  |I love Apache Spark and ML models  |\n",
            "+-----+-----------------------------------+\n",
            "\n",
            "+-----+-----------------------------------+-------------------------------------------+-------------------------------+-------------------------------------------------------------+\n",
            "|label|sentence                           |words                                      |rawFeatures                    |features                                                     |\n",
            "+-----+-----------------------------------+-------------------------------------------+-------------------------------+-------------------------------------------------------------+\n",
            "|0.0  |Hi I heard about Spark             |[hi, i, heard, about, spark]               |(5,[1,3],[3.0,2.0])            |(5,[1,3],[0.0,0.0])                                          |\n",
            "|0.0  |I wish Spark could use case classes|[i, wish, spark, could, use, case, classes]|(5,[0,1,2,3],[2.0,2.0,2.0,1.0])|(5,[0,1,2,3],[1.0216512475319814,0.0,1.8325814637483102,0.0])|\n",
            "|1.0  |Logistic regression models are neat|[logistic, regression, models, are, neat]  |(5,[1,3,4],[2.0,1.0,2.0])      |(5,[1,3,4],[0.0,0.0,1.8325814637483102])                     |\n",
            "|1.0  |I love Apache Spark and ML models  |[i, love, apache, spark, and, ml, models]  |(5,[0,1,3],[1.0,5.0,1.0])      |(5,[0,1,3],[0.5108256237659907,0.0,0.0])                     |\n",
            "+-----+-----------------------------------+-------------------------------------------+-------------------------------+-------------------------------------------------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hmvyR7_5gEFw",
        "outputId": "8e26504b-feff-492b-a198-bf9716d778d9"
      },
      "source": [
        "from pyspark.ml.feature import Tokenizer, RegexTokenizer\n",
        "from pyspark.sql.functions import col, udf\n",
        "from pyspark.sql.types import IntegerType\n",
        "\n",
        "tokenizer = Tokenizer(inputCol=\"sentence\", outputCol=\"words\")\n",
        "\n",
        "regexTokenizer = RegexTokenizer(inputCol=\"sentence\", outputCol=\"words\", pattern=\"\\\\W\")\n",
        "# alternatively, pattern=\"\\\\w+\", gaps(False)\n",
        "\n",
        "countTokens = udf(lambda words: len(words), IntegerType())\n",
        "\n",
        "tokenized = tokenizer.transform(sentenceData)\n",
        "tokenized.select(\"sentence\", \"words\")\\\n",
        "    .withColumn(\"tokens\", countTokens(col(\"words\"))).show(truncate=False)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------------------------------+-------------------------------------------+------+\n",
            "|sentence                           |words                                      |tokens|\n",
            "+-----------------------------------+-------------------------------------------+------+\n",
            "|Hi I heard about Spark             |[hi, i, heard, about, spark]               |5     |\n",
            "|I wish Spark could use case classes|[i, wish, spark, could, use, case, classes]|7     |\n",
            "|Logistic regression models are neat|[logistic, regression, models, are, neat]  |5     |\n",
            "|I love Apache Spark and ML models  |[i, love, apache, spark, and, ml, models]  |7     |\n",
            "+-----------------------------------+-------------------------------------------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Py-ANBo9j-Fw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "179629de-7ca3-4938-dff7-39254878cb1e"
      },
      "source": [
        "from pyspark.ml.feature import StopWordsRemover\n",
        "\n",
        "sentenceData3 = sc.createDataFrame([\n",
        "    (0, [\"I\", \"saw\", \"the\", \"red\", \"balloon\"]),\n",
        "    (1, [\"Mary\", \"had\", \"a\", \"little\", \"lamb\"])\n",
        "], [\"id\", \"raw\"])\n",
        "\n",
        "remover = StopWordsRemover(inputCol=\"raw\", outputCol=\"removeded\")\n",
        "remover.transform(sentenceData3).show(truncate=False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+----------------------------+--------------------+\n",
            "|id |raw                         |removeded           |\n",
            "+---+----------------------------+--------------------+\n",
            "|0  |[I, saw, the, red, balloon] |[saw, red, balloon] |\n",
            "|1  |[Mary, had, a, little, lamb]|[Mary, little, lamb]|\n",
            "+---+----------------------------+--------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L_4bFMO6fcL5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a8c883fd-95c7-403f-89d2-643720f17f92"
      },
      "source": [
        "from pyspark.ml.feature import Normalizer, StandardScaler, MinMaxScaler, MaxAbsScaler\n",
        "\n",
        "from pyspark.ml.linalg import Vectors\n",
        "\n",
        "dataFrame4 = sc.createDataFrame([\n",
        "    (0, Vectors.dense([1.0, 4, 2]),),\n",
        "    (1, Vectors.dense([2.0, 1.0, 1.0]),),\n",
        "    (2, Vectors.dense([4.0, 10.0, 2.0]),)\n",
        "], [\"id\", \"features\"])\n",
        "\n",
        "scaler = MinMaxScaler(inputCol=\"features\", outputCol=\"scaledFeatures\")\n",
        "scaledData = scaler.fit((dataFrame4)).transform(dataFrame4)\n",
        "scaledData.show(truncate=False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+--------------+----------------------------+\n",
            "|id |features      |scaledFeatures              |\n",
            "+---+--------------+----------------------------+\n",
            "|0  |[1.0,4.0,2.0] |[0.0,0.3333333333333333,1.0]|\n",
            "|1  |[2.0,1.0,1.0] |[0.3333333333333333,0.0,0.0]|\n",
            "|2  |[4.0,10.0,2.0]|[1.0,1.0,1.0]               |\n",
            "+---+--------------+----------------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VOYS5zyry8Z0",
        "outputId": "ff241b30-70a3-4e31-a38c-6823e7fa08c1"
      },
      "source": [
        "from pyspark.ml.feature import PCA\n",
        "from pyspark.ml.linalg import Vectors\n",
        "\n",
        "data = [(Vectors.sparse(5, [(1, 1.0), (3, 7.0)]),),\n",
        "        (Vectors.dense([2.0, 0.0, 3.0, 4.0, 5.0]),),\n",
        "        (Vectors.dense([4.0, 0.0, 0.0, 6.0, 7.0]),)]\n",
        "df = sc.createDataFrame(data, [\"features\"])\n",
        "\n",
        "pca = PCA(k=2, inputCol=\"features\", outputCol=\"pcaFeatures\")\n",
        "model = pca.fit(df)\n",
        "\n",
        "result = model.transform(df).select(\"pcaFeatures\")\n",
        "result.show(truncate=False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------------------------------------+\n",
            "|pcaFeatures                             |\n",
            "+----------------------------------------+\n",
            "|[1.6485728230883814,-4.0132827005162985]|\n",
            "|[-4.645104331781533,-1.1167972663619048]|\n",
            "|[-6.428880535676488,-5.337951427775359] |\n",
            "+----------------------------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4lj-U8SxtbTl"
      },
      "source": [
        "sc.stop()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}